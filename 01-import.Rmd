---
title: "Import food desert info"
output:
  html_document:
    df_print: paged
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_dir = "docs") })
---

```{r setup, echo=T, results='hide', message=F, warning=F}
library(tidyverse)
library(janitor)
library(readxl)

```

## Import the data

There are a number of files in `data-processed` that are exports from [Astrid's Workbench](https://app.workbenchdata.com/workflows/39111/).

Though now I'm starting from the original downloaded data in `data-raw`.

- One thought is I can use the xx function i read_excel to pull in only the column we need during import. The values I used with csvcut were: `1-6,10-13,15-24`

```{r}
fa_raw <- read_excel("data-raw/DataDownload2015.xlsx", sheet = "Food Access Research Atlas")
```

Look at names so we can select them:

```{r}
fa_raw %>% names
```

### Select and filter data

- This list based on earlier csvkit work. This could change.
- Filtering to Travis County, Texas

```{r}
fa_travis <- fa_raw %>%
  select(c(1:6,10:13,15:24)) %>% # selects columns by position
  filter(State == "Texas", County == "Travis")

# reducing columns further. removing Low Access only columns
fa_travis_lila <- fa_travis %>% 
  select(-starts_with("LA"))
```

```{r}
fa_travis_lila %>% names()
```

## Plan from here

- Reorder the columns to put LILA at the end?
- Reshape LILA columns into one column
- Change them into factors
- Order by the factors
- Filter to tracts that are true (i.e., find the factor that has GS furthest away)
- Filter to remove factor values that are higher than the first furthest away value. i.e., if there is a story at 1 mile, we don't need 10 mile or 20 mile.
- We should end up with 218 rows again ... one for each tract.


